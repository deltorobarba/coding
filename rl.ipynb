{"nbformat":4,"nbformat_minor":0,"metadata":{"environment":{"name":"tf2-2-3-gpu.2-3.m58","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m58"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"},"colab":{"name":"rl","provenance":[{"file_id":"1fBDxOCdkPtVi78d10-7RJL4pvmaKFJJb","timestamp":1605468623753}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"XOM4j723Kxc2"},"source":["\n","## Google\n","# Applied Contextual Bandits for classification problems using Tensorflow and bigquery\n","\n","**Authors:**  <br>\n","Anant Nawalgaria<br>\n","Alex Erfurt\n","\n","Machine Learning Specialists\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ftcVe8b6Kxc3"},"source":["# Table of Contents"]},{"cell_type":"markdown","metadata":{"id":"sLrwp8njKxc4"},"source":["1. [Initial Setup: installing and importing required libraries](#1.-Initial-Setup:-installing-and-importing-required-libraries)\n","2. [Loading the training dataset from Bigquery to Tensorflow](#2.-Loading-the-training-dataset-from-Bigquery-to-Tensorflow)\n","3. [Initializing and configuring the Environment](#3.-Initializing-and-configuring-the-Environment)\n","4. [Initializing the Agent](#4.-Initializing-the-Agent)\n","5. [Define and link the evaluation metrics](#5.-Define-and-link-the-evaluation-metrics)\n","6. [Initialize & configure the Replay Buffer](#6.Initialize-&-configure-the-Replay-Buffer)\n","7. [Setup and Train the model](#7.-Setup-and-train-the-Model)\n","8. [Inferencing with trained model & Tensorboard Evaluation](#8.-Inferencing-with-trained-model-&-Tensorboard-evaluation)"]},{"cell_type":"markdown","metadata":{"id":"ifdzP2rRKxc5"},"source":["## Introduction\n","\n","\n","Multi-Armed Bandit (MAB) is a Machine Learning framework in which an agent has to select actions (arms) in order to maximize its cumulative reward in the long term. In each round, the agent receives some information about the current state (context), then it chooses an action based on this information and the experience gathered in previous rounds. At the end of each round, the agent receives the reward assiociated with the chosen action.\n","\n","\n",", https://www.tensorflow.org/agents/tutorials/intro_bandit#multi-armed_bandits_and_reinforcement_learning\n","    "]},{"cell_type":"markdown","metadata":{"id":"vpEsA3xmKxc6"},"source":["## 1. Initial Setup: installing and importing required Libraries"]},{"cell_type":"code","metadata":{"id":"fFMdfJoEKxc6"},"source":["!pip install --quiet --upgrade --force-reinstall tensorflow==2.3 tensorflow_probability tensorflow-io --use-feature=2020-resolver\n","!pip install tf_agents gast==0.3.3 --upgrade --use-feature=2020-resolver"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WttMqkUYKxc9"},"source":["import os\n","import numpy as np\n","import functools\n","import os\n","\n","import numpy as np\n","\n","import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n","import tensorflow_probability as tfp\n","from tf_agents.bandits.agents import lin_ucb_agent\n","from tf_agents.bandits.agents import linear_thompson_sampling_agent as lin_ts_agent\n","from tf_agents.bandits.agents import neural_epsilon_greedy_agent as eps_greedy_agent\n","from tf_agents.bandits.agents.examples.v2 import trainer\n","from tf_agents.bandits.environments import classification_environment as ce\n","from tf_agents.bandits.environments import environment_utilities as env_util\n","from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n","from tf_agents.networks import q_network\n","from tf_agents.drivers import dynamic_step_driver\n","from tf_agents.eval import metric_utils\n","from tf_agents.metrics import tf_metrics\n","from tf_agents.policies import policy_saver\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\n","from tf_agents.trajectories import time_step as ts\n","tfd = tfp.distributions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XaRwZ61tKxdA"},"source":["#!gsutil cp covtype.data gs://test_processig/\n","#!gsutil cp Training notebook-Copy1.ipynb gs://test_processig/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"es6Dps6tKxdC"},"source":["## 2. Loading the training dataset from Bigquery to Tensorflow"]},{"cell_type":"markdown","metadata":{"id":"TMn316PUKxdD"},"source":["If you are not already familiar with tf.Dataset, it is recommended to take a few minutes to familiarize yourself with how it works.\n","Some reference material you can find [here](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Tensorflow_io offers you to connect and stream data directly from bigquery: obviating any need to write custom data retrieval and processing routines. You can read about the Tensorflow Bigquery connector [here](https://github.com/tensorflow/io/tree/v0.15.0/tensorflow_io/bigquery). \n","In short we will first initialize a bigqueryclient pointing it to the data source, then make a tf.Dataset out of which using  reads data from parallel from bigquery.parallel_read_rows()."]},{"cell_type":"code","metadata":{"id":"R2ytDmxGKxdD"},"source":["ROOT_DIR = \"/home/jupyter/tmp/quick_test/v7/\"\n","\n","BATCH_SIZE = 128\n","TRAINING_LOOPS = 10\n","STEPS_PER_LOOP = 2\n","AGENT_ALPHA = 10.0\n","\n","EPSILON = 0.01\n","LAYERS = (300, 200, 100, 100, 50, 50)\n","LR = 0.002\n","\n","AGENT_CHECKPOINT_NAME = 'agent'\n","STEP_CHECKPOINT_NAME = 'step'\n","CHECKPOINT_FILE_PREFIX = 'ckpt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wgUIn09HKxdG"},"source":["#sol\n","from tensorflow.python.framework import dtypes\n","from tensorflow_io.bigquery import BigQueryClient\n","from tensorflow_io.bigquery import BigQueryReadSession\n","\n","def features_and_labels(features):\n","  label = features.pop('int64_field_54') # this is what we will train for\n","  \n","  return tf.cast(tf.stack(tf.nest.flatten(features), axis=0), tf.float32), tf.cast(label-1, tf.int32)\n","\n","#COL_NAMES = ['Time', 'Amount', 'Class'] + ['V{}'.format(i) for i in range(1,29)]\n","#COL_TYPES = [dtypes.float64, dtypes.float64, dtypes.int64] + [dtypes.float64 for i in range(1,29)]\n","\n","client = BigQueryClient()\n","num_samples = 581012\n","\n","DATASET_GCP_PROJECT_ID, DATASET_ID, TABLE_ID,  = 'rl-internal-course.training_rl.covtype'.split('.')\n","\n","selected_fields =[\"int64_field_{}\".format(i) for i in range(55)]\n","output_types =  [ dtypes.int64 for field in selected_fields ]\n","\n","bqsession = client.read_session(\n","    \"projects/\" + DATASET_GCP_PROJECT_ID,\n","    DATASET_GCP_PROJECT_ID, TABLE_ID, DATASET_ID,\n","    selected_fields, output_types)\n","\n","tf_dataset = bqsession.parallel_read_rows(block_length = num_samples, num_parallel_calls=tf.data.experimental.AUTOTUNE).map(features_and_labels).repeat().shuffle(buffer_size=400000)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zZM8zjfrKxdI"},"source":["tf_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"do58umOOKxdL"},"source":["## 3. Initializing and configuring the Environment\n","\n","An environment in the TF-Agents Bandits library is a class that provides observations and reports rewards based on obseravtions and actions.\n","In this section we instantiate the \"covertype bandit environment\". Originally, the covertype dataset is a set of labeled examples for different types of forests, taken from here: https://archive.ics.uci.edu/ml/datasets/covertype. To cite the source:\n","\n","\"Predicting forest cover type from cartographic variables only (no remotely sensed data). The actual forest cover type for a given observation (30 x 30 meter cell) was determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. Independent variables were derived from data originally obtained from US Geological Survey (USGS) and USFS data.\"\n","\n","In the TF-Agents bandits library, there is an environment wrapper (named ClassificationBanditEnvironment) that can turn any multiclass labeled dataset to a bandit environment. The context (or observation) will be the features in the dataset, the actions are the label classes, and the rewards are calculated based on some stochastic function of the actual and the guessed labels. This latter function is defined by a table of distributions. For our covertype example, this table is simply the deterministic identity matrix:"]},{"cell_type":"code","metadata":{"id":"TN0ECw1-KxdL"},"source":["# initialize the distribution\n","covertype_reward_distribution = tfd.Independent(\n","    tfd.Deterministic(tf.eye(7)), reinterpreted_batch_ndims=2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8F6MuC3KxdN"},"source":["covertype_reward_distribution.sample()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jkxCINhtKxdQ"},"source":["# provides an interface to return a reward given an action for features, tf_dataset provides labels"]},{"cell_type":"code","metadata":{"id":"eIBnUkTAKxdQ"},"source":["# Initializing the Classification Bandit Environment with the dataset, and reward distri ution\n","environment = ce.ClassificationBanditEnvironment(\n","    tf_dataset, covertype_reward_distribution, BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HHpGGmPCKxdS"},"source":["# \n","environment.reward_spec()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cv5svfBfKxdU"},"source":["## 4. Initializing the Agent\n","Now that we have the environment and metrics intialized from the Tf.dataset loaded from big query we reach the part where we define and initialize our policy and the Agent which will be our utilize that policy to make decisions given an observation. We have several policies: as shown here:\n","\n","   1. [NeuralEpsilonGreedyAgent](https://medium.com/analytics-vidhya/the-epsilon-greedy-algorithm-for-reinforcement-learning-5fe6f96dc870): The neural episilon greed algorithm makes a value estimate for all the arms, and then chooses the best arm with the probaility (1-episilon) and any of the random arms with a probability of epsilon. this balances the exploration-exploitation tradeoff and epsilon is set to a small value like 10%. Example: In this example we have seven arms: one of each of the classes, and if we set episilon to say 10%, then 90% of the times the agent will choose the arm with the highest value estimate ( expplotiing the one most likely to be the predicted class) and 10% of the time it will choose a random arm from all of the 7 arms( thus exploring the other possibilities). Refer [here](https://www.tensorflow.org/agents/api_docs/python/tf_agents/bandits/agents/neural_epsilon_greedy_agent/NeuralEpsilonGreedyAgent) for more information of the tensorflow agents version of the same.\n","   \n","   Each Agent is initiatlizied with a policy: which is essentially the function approximator ( be it linear or non linear) for estimating the Q values. Ther agen uses this policy, adds the exploration-exploitation component on top of this and is then trained. In this example we will use a Deep Q Network as our policy\n"]},{"cell_type":"code","metadata":{"id":"_fGaJF6mKxdV"},"source":["network = q_network.QNetwork(\n","          input_tensor_spec=environment.time_step_spec().observation,\n","          action_spec=environment.action_spec(),\n","          fc_layer_params=LAYERS)\n","\n","agent = eps_greedy_agent.NeuralEpsilonGreedyAgent(\n","  time_step_spec=environment.time_step_spec(),\n","  action_spec=environment.action_spec(),\n","  reward_network=network,\n","  optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n","  epsilon=EPSILON)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CYsBS9eRKxdX"},"source":["## 5. Define and link the evaluation metrics\n","\n","\n","Just like you have metrics like accuracy/recall in supervised learning, in bandits we use the [regret](https://www.tensorflow.org/agents/tutorials/bandits_tutorial#regret_metric) metric per episode. To calculate the regret, we need to know what the highest possible expected reward is in every time step. For that, we define the `optimal_reward_fn`.\n","\n","Another similar metric is the number of times a suboptimal action was chosen. That requires the definition if the `optimal_action_fn`."]},{"cell_type":"code","metadata":{"id":"-l0e78osKxdX"},"source":["optimal_reward_fn = functools.partial(\n","    env_util.compute_optimal_reward_with_classification_environment,\n","    environment=environment)\n","\n","optimal_action_fn = functools.partial(\n","    env_util.compute_optimal_action_with_classification_environment,\n","    environment=environment)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yXSX61X5Kxda"},"source":["regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n","suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n","    optimal_action_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DynX8nIBKxdc"},"source":["\n","step_metric = tf_metrics.EnvironmentSteps()\n","metrics = [tf_metrics.NumberOfEpisodes(),  #equivalent to number of steps in bandits problem\n","           regret_metric,  # measures regret\n","           suboptimal_arms_metric,  # number of times the suboptimal arms are pulled\n","           tf_metrics.AverageReturnMetric(batch_size=environment.batch_size)  # the average return\n","           ]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qi5VVgkeKxde"},"source":["## 6. Initialize & configure the Replay Buffer\n","Reinforcement learning algorithms use replay buffers to store trajectories of experience when executing a policy in an environment. During training, replay buffers are queried for a subset of the trajectories (either a sequential subset or a sample) to \"replay\" the agent's experience. Sampling from the replay buffer facilitate data re-use and breaks harmful co-relation between sequential data in RL, although in contextual bandits this isn't absolutely required but still helpful.\n","\n","The replay buffer exposes several functions which allow you to manipulate the replay buffer in several ways. Read more on them [here] (https://www.tensorflow.org/agents/tutorials/5_replay_buffers_tutorial)\n","\n","In this demo we would be using the TFUniformReplayBuffer for which we need to initialize the buffer spec with the spec of the trajectory of the agent's policy, a chosen batch size( number of trajectories to store), and the maximum length of the trajectory. ( this is the amount of sequential time steps which will be considered as one data point). so a batch of 3 with 2 time steps each would result in a tensor of shape (3,2). Since unlike regular RL problems, Contextual bandits have only one time step we can keep max_length =1, however since this tutorial is to enable you for RL problems as well, let set it to 2. Do not worry, any contextual bandit agent will internally\n","split the time steps inside each data point such that the effective batch size ends up being (6,1). "]},{"cell_type":"markdown","metadata":{"id":"4P0uSb9eKxdf"},"source":["Create a Tensorflow based UniformReplayBuffer And initialize it with an appropriate values.\n","Recommended:\n","    Batch size= 128\n","    Max length = 2 ( 2 time steps per item)"]},{"cell_type":"code","metadata":{"id":"WcsLSlfDKxdf"},"source":["# solution\n","buf = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n","      data_spec=agent.policy.trajectory_spec,\n","      batch_size=BATCH_SIZE,\n","      max_length=STEPS_PER_LOOP)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RYYbzaPuKxdh"},"source":["Now we have a Replay buffer but we also need something to fill it with. Often a common practice is to have \n","the agent Interact with and collect experience with the environment, without actually learning from it ( i.e. only forward pass). This loop can  be either by you manually as shown [here](https://www.tensorflow.org/agents/tutorials/6_reinforce_tutorial#training_the_agent) or you can do it using the DynamicStepDriver.\n","The data encountered by the driver at each step is saved in a named tuple called Trajectory and broadcast to a set of observers such as replay buffers and metrics. \n","This Trajectory includes the observation from the environment, the action recommended by the policy, the reward obtained, the type of the current and the next step, etc. \n","\n","In order for the driver to fill the replay buffer with data, as well as to compute ongoing metrics, it needs acess to the add_batch, funcitonality of the buffer, and the metrics ( both step and regular). Refer [here](https://www.tensorflow.org/agents/tutorials/5_replay_buffers_tutorial#data_collection) for more information aand example code on how initialize a step driver with observers. \n"]},{"cell_type":"code","metadata":{"id":"-yUkG51PKxdh"},"source":["# solution\n","replay_observer = [buf.add_batch, step_metric] + metrics  #  UNCOMMENT THIS BACK WHEN DEBUGGING IS DONE (GABOR)\n","# replay_observer = [buf.add_batch]  # Gabor's debug\n","\n","driver = dynamic_step_driver.DynamicStepDriver(\n","      env=environment,\n","      policy=agent.collect_policy,\n","      num_steps=STEPS_PER_LOOP * environment.batch_size,\n","      observers=replay_observer )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"leUtw6DDKxdk"},"source":[" Here we provide you a helper function in order to save your agent, the metrics and its lighter policy seperately, while training the model. We make all the aspects into trackable objects and then use checkpoint to save as well warm restart a previous training. For more information on checkpoints and policy savers ( which will be used in the training loop below) refer [here](https://www.tensorflow.org/agents/tutorials/10_checkpointer_policysaver_tutorial)"]},{"cell_type":"code","metadata":{"id":"mWELf3aPKxdk"},"source":["def restore_and_get_checkpoint_manager(root_dir, agent, metrics, step_metric):\n","  \"\"\"Restores from `root_dir` and returns a function that writes checkpoints.\"\"\"\n","  trackable_objects = {metric.name: metric for metric in metrics}\n","  trackable_objects[AGENT_CHECKPOINT_NAME] = agent\n","  trackable_objects[STEP_CHECKPOINT_NAME] = step_metric\n","  checkpoint = tf.train.Checkpoint(**trackable_objects)\n","  checkpoint_manager = tf.train.CheckpointManager(checkpoint=checkpoint,\n","                                                  directory=root_dir,\n","                                                  max_to_keep=5)\n","  latest = checkpoint_manager.latest_checkpoint\n","\n","  if latest is not None:\n","    print('Restoring checkpoint from %s.', latest)\n","    checkpoint.restore(latest)\n","    print('Successfully restored to step %s.', step_metric.result())\n","  else:\n","    print('Did not find a pre-existing checkpoint. '\n","                 'Starting from scratch.')\n","  return checkpoint_manager\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6ttQ0RjKxdm"},"source":["checkpoint_manager = restore_and_get_checkpoint_manager(\n","  ROOT_DIR, agent, metrics, step_metric)\n","# saver = policy_saver.PolicySaver(agent.policy)\n","summary_writer = tf.summary.create_file_writer(ROOT_DIR)\n","summary_writer.set_as_default()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nPJBV8WjKxdo"},"source":["Now we have all the components ready to start training the model. Here is the process for Training the model\n","1. We first use the DynamicStepdriver instance to collect experience( trajectories) from the environment and fill up the replay buffer.\n","2. We then extract all the stored experience from the replay buffer by specfiying the batch size and num_steps the same as we initialized the driver with. We extract it as tf.dataset instance.\n","3. We then iterate on the tf.dataset and the first sample we draw actually has all the data batch_size*num_time_steps\n","4. the agent then trains on the acquired experience\n","5. the replay buffer is cleared to make space for new data\n","6. Log the metrics and store them on disk\n","7. Save the Agent ( via checkpoints) as well as the policy"]},{"cell_type":"code","metadata":{"id":"hryNDsarKxdo"},"source":["#solution\n","import warnings\n","warnings.filterwarnings('ignore')\n","TRAINING_LOOPS = 15000\n","\n","for _ in range(TRAINING_LOOPS):\n","    driver.run()\n","    batch_size = driver.env.batch_size\n","    \n","    dataset = buf.as_dataset(\n","      \n","        sample_batch_size = BATCH_SIZE,\n","        num_steps=STEPS_PER_LOOP,\n","        single_deterministic_pass=True)\n","\n","    experience, unused_info = next(iter(dataset))\n","    train_loss = agent.train(experience).loss\n","    buf.clear()\n","    metric_utils.log_metrics(metrics)\n","        # for m in metrics:\n","        # print(m.name, \": \", m.result())\n","    for metric in metrics:\n","        metric.tf_summaries(train_step=step_metric.result())\n","    checkpoint_manager.save()\n","    \n","    # saver.save(os.path.join(ROOT_DIR, \"./\", 'policy_%d' % step_metric.result()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pYlG8vJcKxdq"},"source":["Now that our model is trained, what if we want to determine which action to take given a new \"context\": for that we will iterate on our dataset to get the next item,\n","    make a timestep out of it by wrapping the results using ts.Timestep. It expects step_type, reward, discount, and observation as input: since we are performing prediction you can fill \n","        in dummy values for the first 3: only the observation/context is relevant. Read about how it works [here](https://www.tensorflow.org/agents/api_docs/python/tf_agents/trajectories/time_step/TimeStep), and perform the task below\n","        \n","       "]},{"cell_type":"code","metadata":{"id":"SNwKJjdpKxdr"},"source":["feature, label = iter(tf_dataset).next()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mv6cTWieKxds"},"source":["step = ts.TimeStep(\n","        tf.constant(\n","            ts.StepType.FIRST, dtype=tf.int32, shape=[1],\n","            name='step_type'),\n","        tf.constant(0.0, dtype=tf.float32, shape=[1], name='reward'),\n","        tf.constant(1.0, dtype=tf.float32, shape=[1], name='discount'),\n","        tf.constant(feature,\n","                    dtype=tf.float32, shape=[1,54],\n","                    name='observation'))\n","\n","agent.policy.action(step).action.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4f77CYkOKxdv"},"source":["One final task : let us upload the tensoboard logs, to get an overview of the performance of our model. We will upload our logs to tensorboard.dev and for that you need to \n","copy the following command in terminal and execute it from there, it will give you a link from which you need to copy/paste the authentication code, and once that is done, you will receive the \n","url of your model evaluation, hosted on a public [tensorboard.dev](https://tensorboard.dev/) instance"]},{"cell_type":"code","metadata":{"id":"0nkHrPBPKxdv"},"source":["!tensorboard dev upload --logdir /home/jupyter/tmp/quick_test/v7/ --name \"(optional) My latest experiment\" --description \"(optional) Agent trained\""],"execution_count":null,"outputs":[]}]}