{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMY6+FiloVMVFcqQsCqUQPo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/main/llm_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM Evaluation**"
      ],
      "metadata": {
        "id": "5NgsyFB7RIf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Blogposts**\n",
        "\n",
        "* https://cloud.google.com/blog/products/ai-machine-learning/evaluating-large-language-models-in-business?hl=en\n",
        "\n",
        "* https://cloud.google.com/blog/products/ai-machine-learning/enhancing-llm-quality-and-interpretability-with-the-vertex-gen-ai-evaluation-service/?hl=en\n",
        "\n",
        "* https://medium.com/google-cloud/vqa-3-how-to-evaluate-generated-answers-from-rag-at-scale-on-vertex-ai-70bc397cb33d\n",
        "\n",
        "* Video: [Beyond recall: Evaluating Gemini with Vertex AI Auto SxS](https://www.youtube.com/live/ysvjuAPY8xs)\n",
        "\n"
      ],
      "metadata": {
        "id": "6vL7kN2bQ737"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Technical Documentation**\n",
        "\n",
        "* Notebooks: https://github.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/evaluation\n",
        "\n",
        "* https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview\n",
        "\n",
        "  * https://cloud.google.com/vertex-ai/generative-ai/docs/models/side-by-side-eval\n",
        "\n",
        "  * https://cloud.google.com/vertex-ai/generative-ai/docs/models/computation-based-eval-pipeline"
      ],
      "metadata": {
        "id": "mLRHpFbkQ-Mr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Vertex AI SDK for evaluating a summarization task (with Computation-based metrics)**\n",
        "\n",
        "(The code uses metric bundles for evaluating a summarization task and it automatically logs evaluation parameters and metrics in Vertex AI Experiments)"
      ],
      "metadata": {
        "id": "-Ocsw9fZS9Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview.evaluation import EvalTask\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "\n",
        "summarization_eval_task = EvalTask(\n",
        "    dataset=EVAL_DATASET,\n",
        "    metrics=[\n",
        "        \"text_generation_quality\",\n",
        "        \"text_generation_factuality\",\n",
        "        \"text_generation_instruction_following\",\n",
        "        \"summarization_pointwise_reference_free\",\n",
        "    ],\n",
        "    experiment=\"generative-ai-eval-experiment\",\n",
        ")\n",
        "\n",
        "prompt_templates = [\n",
        "    \"Instruction: {instruction}. Article: {context}. Summary:\",\n",
        "    # Provide a list of prompt templates to evaluate and compare.\n",
        "    ...\n",
        "]\n",
        "\n",
        "eval_results = []\n",
        "for i, prompt_template in enumerate(prompt_templates):\n",
        "    eval_result = summarization_eval_task.evaluate(\n",
        "        model=GenerativeModel(\"gemini-1.5-pro\"),\n",
        "        prompt_template=prompt_template,\n",
        "        experiment_run_name=f\"eval-run-prompt-{i}\",\n",
        "    )\n",
        "    eval_results.append(\n",
        "        (f\"Prompt #{i}\", eval_result.summary_metrics, eval_result.metrics_table)\n",
        "    )"
      ],
      "metadata": {
        "id": "ikGLa6IBS2OC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}