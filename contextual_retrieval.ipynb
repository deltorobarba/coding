{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfPa9w5yali3vGck6SIoY2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/main/contextual_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Contextual Retrieval**"
      ],
      "metadata": {
        "id": "wFAz5fYLBCPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Highly appreciated and enjoyed the clarity of this post from Anthropic on how to improve RAG quality, combining multiple techniques.\n",
        "\n",
        "* It's a great high-level summary of a bunch of related topics - BM25, embeddings, reranking, evals, etc. **But the main novelty of this post is the use of additional document-level context as\"metadata\" for every chunk**. Moreover, they create high quality dynamic contextual metadata, by using Claude Heiku  to generate that context, during chunking and indexing time. To achieve that they use a simple prompt with the chunk, the (sub) document, and a simple instruction:\n",
        "\n",
        "<document>\n",
        "{{WHOLE_DOCUMENT}}\n",
        "</document>\n",
        "Here is the chunk we want to situate within the whole document\n",
        "<chunk>\n",
        "{{CHUNK_CONTENT}}\n",
        "</chunk>\n",
        "\n",
        "* Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.\n",
        "\n",
        "* In the not so distant past, this would have been very expensive, resending the whole document to the model for every chunk. But with the recently launched caching feature, it's quite adorable:\n",
        "\n",
        "* \"Assuming 800 token chunks, 8k token documents, 50 token context instructions, and 100 tokens of context per chunk, the one-time cost to generate contextualized chunks is $1.02 per million document tokens.\""
      ],
      "metadata": {
        "id": "ge6Fm0PKBI7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.anthropic.com/news/contextual-retrieval\n"
      ],
      "metadata": {
        "id": "nq-Td8v8BFRx"
      }
    }
  ]
}