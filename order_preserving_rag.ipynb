{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+Wobaa5joNVsKQ8hrz/yR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deltorobarba/machinelearning/blob/main/order_preserving_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Order-preserving RAG**"
      ],
      "metadata": {
        "id": "EDaeDu-uLcZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Das lässt manche an der Notwendigkeit von Retrieval-Augmented Generation (RAG) zweifeln. Bei RAG werden Informationen üblicherweise zunächst in einer Vektordatenbank gespeichert und dynamisch bei jeder Anfrage abgerufen.\n",
        "\n",
        "* Eine neue Studie von Nvidia-Forschenden stellt diese Annahme, dass man RAG mit wachsenden Kontextfenstern nicht brauchen würde, in Frage. Sie zeigt, dass RAG in Kombination mit einem bestimmten Mechanismus LLMs mit großen Kontextfenstern übertreffen kann.\n",
        "\n",
        "* Das Paper \"In Defense of RAG in the Era of Long-Context Language Models\" schlägt konkret einen reihenfolgeerhaltenden RAG-Ansatz (OP-RAG, OP steht für order-preserving) vor, der die ursprüngliche Reihenfolge der abgerufenen Teile (Chunks) im LLM-Kontext beibehält. Dies steht im Gegensatz zu herkömmlichen RAG-Methoden, die die Chunks meist nach absteigender Relevanz anordnen.\n",
        "\n",
        "* **Die Studie untersuchte auch den Einfluss der Kontextlänge auf die Leistung von OP-RAG. Die Ergebnisse zeigten, dass die Antwortqualität mit zunehmender Kontextlänge zunächst verbessert, dann aber abnahm.**\n",
        "\n",
        "* Paper: [In Defense of RAG in the Era of Long-Context Language Models](https://arxiv.org/abs/2409.01666)\n",
        "\n",
        "* https://the-decoder.de/studie-laesst-zweifel-an-nutzen-grosser-kontextfenster-aufkommen/"
      ],
      "metadata": {
        "id": "MimSGh8FLhAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import faiss\n",
        "\n",
        "# Load your LLM\n",
        "model_name = \"your-llm-model\" # Replace with your model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Load your knowledge base (replace with your own data)\n",
        "dataset = load_dataset(\"your-dataset\")\n",
        "\n",
        "# Create a FAISS index for efficient retrieval\n",
        "index = faiss.IndexFlatL2(768)  # Assuming 768-dimensional embeddings\n",
        "passages = [example['text'] for example in dataset['train']]\n",
        "index.add(passages)\n",
        "\n",
        "# Order-preserving RAG function\n",
        "def order_preserving_rag(query, k=3):\n",
        "    # Retrieve relevant passages\n",
        "    _, passage_indices = index.search(query, k)\n",
        "\n",
        "    # Keep the original order of the retrieved passages\n",
        "    retrieved_passages = [passages[i] for i in passage_indices[0]]\n",
        "\n",
        "    # Combine the query and retrieved passages for the LLM\n",
        "    context = f\"Query: {query}\\nContext: {' '.join(retrieved_passages)}\"\n",
        "\n",
        "    # Generate a response\n",
        "    response = generator(context, max_length=100)\n",
        "\n",
        "    return response\n",
        "\n",
        "# Example usage\n",
        "query = \"What is the capital of France?\"\n",
        "response = order_preserving_rag(query)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "VL9pi0vsM5Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Load LLM and Knowledge Base:**\n",
        "   - Replace `'your-llm-model'` and `'your-dataset'` with your actual LLM and dataset.\n",
        "   - We're assuming a dataset with a 'text' field containing the knowledge base passages.\n",
        "\n",
        "2. **FAISS Index:**\n",
        "   - FAISS is used for efficient similarity search to retrieve relevant passages.\n",
        "   - You'll likely need to preprocess your data and generate embeddings before creating the index.\n",
        "\n",
        "3. **`order_preserving_rag` Function:**\n",
        "   - Takes a `query` and optionally the number of passages to retrieve (`k`).\n",
        "   - Retrieves relevant passages using FAISS, maintaining their original order.\n",
        "   - Combines the query and retrieved passages into a context for the LLM.\n",
        "   - Generates a response using the LLM.\n",
        "\n",
        "4. **Example Usage:**\n",
        "   - Demonstrates how to use the function to answer a question.\n",
        "\n",
        "**Notes**\n",
        "\n",
        "- **Order Preservation:** The core idea is to keep the retrieved passages in the same order they appear in the knowledge base. This can be crucial for tasks where the order of information matters (e.g., summarizing a historical event).\n",
        "- **FAISS:** FAISS is used for efficient retrieval, but you might need to adapt it to your specific embedding method.\n",
        "- **LLM Prompting:** The way you combine the query and retrieved passages in the context can significantly impact the LLM's performance. Experiment with different prompt formats.\n",
        "\n",
        "**Considerations**\n",
        "\n",
        "- **Preprocessing and Embeddings:** You'll need to preprocess your knowledge base and generate embeddings suitable for FAISS.\n",
        "- **Error Handling:** Add error handling for cases where retrieval fails or the LLM generates unexpected responses.\n",
        "- **Evaluation:** Carefully evaluate the performance of your Order-preserving RAG system on your specific task."
      ],
      "metadata": {
        "id": "nfevZOVMM8iE"
      }
    }
  ]
}